\section{State-of-the-art protein language modeling}
It is intuitive to represent a protein as a sequence of letters with each letter corresponding to an amino acid. As with natural languages, we can find common elements between naturally evolved proteins. Noticeable patterns reoccurring in multiple (related) protein sequences are highly likely to be biologically relevant. These motifs and domains are essential to many biological processes and can easily be represented as words, phrases or sentences of amino acids in a language model perspective. This is why researchers are taking inspiration from the recent successes of natural language processing (NLP) and applying this to a biological context. NLP is a branch of artificial intelligence (AI) concerning itself with creating the ability for computers to learn and understand human languages by using statistical, machine learning and in recent years deep learning models. Common tasks in NLP which have been extended into protein sequence research include part-of-speech tagging, named enitity recognition and natural language generation.

As with protein modeling, applying labels to millions of web pages, articles, journals etc.\ is a labor-intensive procedure and thus state-of-the-art NLP models use a form of \textit{self-supervised learning}.