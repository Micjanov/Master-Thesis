\chapter{Conclusion}
First, we discussed the history and advancements in molecular biology and bioinformatics made in the mid-20th 21st century that revolutionized our understanding of the genetic code, sequence analyses and protein structures. In particular, the integration of natural language processing (NLP) techniques into protein research that has further expanded the horizons of bioinformatics.

In response to the need for more efficient algorithms and learning methods in the realm of protein language modeling, we presented hyperdimensional computing, a concept inspired by the human brain's capacity to learn, adapt and recognize semantic relationships. The power of this computing framework was demonstrated by illustrating its operations and applying it to several datasets.

In our exploration of the potential applications of hyperdimensional computing in protein research, we have set a workflow in chapter 3. This process involved encoding amino acids and protein sequences into hyperdimensional embeddings, subsequently leveraging these embeddings in prediction tasks. Our initial focus centered on identifying appropriate encoding mechanisms for both amino acids and proteins. Multiple strategies for each were illustrated and evaluated using visual projections. Notably, we observed that existing amino acid embeddings, such as those from the ESM-2 model, proved to be useful in amino acid encodings. Moreover, we ventured into exploring alternative methods that could potentially enhance our study. Yet, as we navigated through this process, the issue of vectors potentially becoming oversaturated surfaced, which highlighted the need for further research in this area.

We then applied these amino acid- and protein-encoding methods to encode sequences of the PhaLP database. On top of this, we explored a multitude of methods to learn and predict classes of these sequences using the generated embeddings. We found a hyperdimensional computing-based method, OnlineHD, to be particularly performing well on classification tasks on our sequence embeddings, achieving competitive performance levels compared to other established machine learning methodologies in some cases. This substantiated the effectiveness of our sequence encoding methods, as the models were able to accurately learn and predict from these embeddings.

In chapter 5, we focused on our exploration of perceptron-based models for context-aware protein residue learning, utilizing neighborhood-encoded hyperdimensional vectors as an input. Our motivation for this endeavor was to explore for potential avenues to design computationally efficient alternatives to transformer-based models, which, despite their state-of-the-art performance, impose significant computational demands.

Our methodology involved training different perceptron-based architectures (specifically, single-layer perceptron (SLP) and multi-layer perceptron (MLP) models) on Netsurf 2.0's training dataset. The experiments with these models led to poor results, revealing insights into the performance of perceptron models with high-dimensional inputs. This exploration, while not leading to a model that outperforms existing state-of-the-art models, has provided insights into the challenges of designing efficient models for context-aware protein residue learning.