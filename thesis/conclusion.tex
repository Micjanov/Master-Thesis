\chapter{Conclusion}
First, we discussed the history and advancements in molecular biology and bioinformatics made in the mid-20th 21st century that revolutionized our understanding of the genetic code, protein structure, and sequence analysis. In particular, the integration of natural language processing (NLP) techniques into protein research that has further expanded the horizons of bioinformatics.

In chapter 5, we focused on our exploration of perceptron-based models for context-aware protein residue learning, utilizing neighborhood-encoded hyperdimensional vectors as an input. Our motivation for this endeavor was to explore for potential avenues to design computationally efficient alternatives to transformer-based models, which, despite their state-of-the-art performance, impose significant computational demands.

Our methodology involved training different perceptron-based architectures (specifically, single-layer perceptron (SLP) and multi-layer perceptron (MLP) models) on Netsurf 2.0's training dataset. The experiments with these models led to varied results, revealing insights into the performance of perceptron models with high-dimensional inputs. This exploration, while not leading to a model that outperforms existing state-of-the-art models, has provided insights into the challenges of designing efficient models for context-aware protein residue learning. ADD