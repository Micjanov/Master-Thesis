\chapter{Conclusion and future perspectives}
This thesis has investigated the potential applications of hyperdimensional computing in the field of bioinformatics, specifically protein sequence research. Starting from an overview of protein research, we identified the need for more efficient computational methods. Responding to this need, we explored the principles of hyperdimensional computing and developed encoding strategies for amino acids and protein sequences. We tested these strategies on the PhaLP protein sequence database and used the resulting embeddings in prediction tasks. Our findings indicate potential benefits of these methods, despite some challenges, and provide a foundation for further research in this area.

First in Chapter 1, we discussed the history and advancements in molecular biology and bioinformatics made in the mid-20th 21st century that revolutionized our understanding of the genetic code, sequence analyses and protein structures. In particular, the integration of natural language processing (NLP) techniques into protein research that has further expanded the horizons of bioinformatics.

In response to the need for more efficient algorithms and learning methods in the realm of protein language modeling, we presented hyperdimensional computing, a concept inspired by the human brain's capacity to learn, adapt and recognize semantic relationships. The power of this computing framework was demonstrated by illustrating its operations and applying it to several datasets.

In our exploration of the potential applications of hyperdimensional computing in protein research, we have set a workflow in Chapter 3. This process involved encoding amino acids and protein sequences into hyperdimensional embeddings, subsequently leveraging these embeddings in prediction tasks. Our initial focus centered on identifying appropriate encoding mechanisms for both amino acids and proteins. Multiple strategies for each were illustrated and evaluated using visual projections. Notably, we observed that existing amino acid embeddings, such as those from the ESM-2 model, proved to be useful as starting points for amino acid encodings. Moreover, we ventured into exploring alternative methods that could potentially enhance our study.

We then applied these amino acid- and protein-encoding methods to encode sequences of the PhaLP database in Chapter 4. On top of this, we explored a multitude of methods to learn and predict classes of these sequences using the generated embeddings. We found a hyperdimensional computing-based method, OnlineHD, to be particularly performing well on classification tasks on our sequence embeddings, achieving competitive performance levels compared to other established machine learning methodologies in some cases. This substantiated the effectiveness of our sequence encoding methods, as the models were able to accurately learn and predict from these embeddings.

In Chapter 5, we focused on our exploration of perceptron-based models for context-aware protein residue learning, utilizing neighborhood-encoded hyperdimensional vectors as an input. Our motivation for this endeavor was to explore for potential avenues to design computationally efficient alternatives to transformer-based models, which, despite their state-of-the-art performance, impose significant computational demands. Our methodology involved training different perceptron-based architectures (specifically, single-layer perceptron and multi-layer perceptron models) on NetsurfP 2.0's training dataset. The experiments with these models led to poor results, revealing insights into the performance of perceptron models with high-dimensional inputs. This exploration, while not leading to a model that outperforms existing state-of-the-art models, has provided insights into the challenges of designing efficient models for context-aware protein residue learning within the framework of hyperdimensional computing.

As we navigated through these processes, the issue of vectors potentially becoming oversaturated surfaced depending on the method used. We saw that prediction methods such as OnlineHD and XGBoost classifiers capture the information in hyperdimensional sequence embeddings very well, but that naively bundled vectors of sequences or neighborhood-encoded amino acid vectors lost most of the input information, which highlights the need for further research in this area.

As for future perspectives, there are many possibilities that could be taken on from here. If saturation of bundling vectors arises as a problem, algorithms could be developed to mitigate this problem. An innovative approach would be to consider the development of end-to-end methods, where the dimensionality and the maximum bundling capacity are integrated as hyperparameters. This could enhance both the flexibility and performance of the system.

Another promising avenue lies in the fusion of successful concepts from state-of-the-art methods, such as self-supervision, pretraining, and self-attention, with principles inherent to hyperdimensional computing. This synergy might result in a new generation of effective protein language modeling tools.

In expanding upon the fundamental operations of hyperdimensional computing, one could focus on the creation of more interpretable models. Given the inherently reversible nature of bundling and binding operations in hyperdimensional computing, one approach could involve leveraging these unique properties to construct highly interpretable networks using HDVs. This could offer a significant advantage over current models which are often referred to as "black boxes" due to their lack of interpretability.

Further exploration in this area could help to unlock the full potential of hyperdimensional computing in bioinformatics and could provide valuable insights for other fields where high-dimensional data and complex patterns are prevalent.