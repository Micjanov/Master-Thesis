\chapter[Introduction]%
{Introduction}

%% Introduction
%%%%%%%%%%%%%%%
\section{Digital biology, protein sequence research and traditional bioninformatics tools}

\section{State-of-the-art, deep learning and protein language modeling}

\section{Hyperdimensional computing}
Hyperdimensional computing is a relatively new paradigm of computing developed by Kanerva.\cite{Kanerva2009} that tries to mimick the workings of a (human) brain by computing with vectors of tens of thousands elements long, so in the realm of hyperdimensionality. The human brain consists of about 100 billion neurons (nerve cells) and 1000 trillion synapes that connect these neurons. Each neuron is connected up to 10000 other neurons, creating massive circuits. This is likely fundamental to the workings of the human brain and what seperates our brains from modern  von Neumann computer architectures which operate on 8 to 64 bit vectors. This becomes clear when we compare the relative simplicity for a human to learn a language compared to computers which require a large and complecated set of arithmetic operations in the form of deep learning networks together with terabytes of data and thousands of Watts of compute power to try to come close mastering a language whilst a human can recognize other languages relatively easy when they don't even speak it. Likewise languages, we can very easily memorize and compare other intrisically complex and contextual concepts such as images. A computer would have a hard time finding similarity between a set of images and faces because this requires very complex machine learning models. The human brain can do this all with a huge efficiency by uconsuming only roughly 20 W of energy.

To achieve this kind of flexible intelligence, we might have to step away from the restrictive von Neumann architecture and so Kanerva proposes the use of hyperdimensional vectors, a different form of representation for entities by which a computer can compute with. The use of models based on high dimensionality is not entirely new and is being explored since the 1990s. Some of these earlier models include Holographic Reduced Representations\cite{HRR}, Spatter Code\cite{spatter} etc. An HDV can represent anything from a scalar number to any kind of concept. This vector is initially made up of totally random elements, but with a simple set of operations which will be explained later, we can use other vectors to combine some concepts into new similar or dissimilar concepts. For example to show the essence of HDC and how it tries to simulate the brain, we can compare the concept of a \textit{table} to the concept of a \textit{brocolli}. We would not immediately conclude that they are in any way similar but as humans we can trace back \textit{table} to \textit{plate} which has some similarities with \textit{food} from which we can easily extract the concept of \textit{brocolli}. These kind of operations are not very obvious for a classical computer but easy for us humans. 

The elements in an HDV can be made up from binary bits like in classical computing but also of bipolar or real numbers. The choice of the nature of the elements has also implications on the nature of the different operations and on the results. Highly efficient bit operations could be used on binary vectors but then the amount of information stored in such a vector would be drastically lessened compared to bipolar or real vectors, leading to a lower accuracy. 

An initial HDV is made up totally random. This \textit{holistic} or \textit{holographic} representation of a concept smeared out over a vector consisting of thousands of bits gives rise to interesting properties of this paradigm such as its robustness. These kind of systems are very tolerant to noise and failure of bits since we introduce a lot of redundancy in the vector just by stochastics. This is very unlike classical computing where every bit counts and one failure in a bit can lead to disasters. Again, this 