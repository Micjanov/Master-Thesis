\chapter[Materials and methods]{Materials and methods}
Every random hyperdimensional vector produced is made to be binary and 10000-dimensional unless noted otherwise. 

\section{Amino acid embeddings}
To encode biological information into the vectors, we used the last layer of the 3 billion-parameter ESM-2 model~\cite{esm2} of every amino acid, resulting in 1024-dimensional real-valued embeddings for every amino acid. To extend these into hyperdimensionality, a simple matrix multiplication has been employed: $A_{1x1024} \times B_{1024x10000} = C_{1x10000}$ where $A$ is an 1024-dimensional ESM-2 embedding and B a matrix of 1024 random 10000-D vectors. The resulting vectors are then min-max scaled and rounded depending on the desired nature of the vectors.

\section{Case study: PhaLP dataset}
As of March 2023, the latest version of the PhaLP database,~\textit{v2021\_04}, has been used to test our models.

\subsection*{Sequence embeddings}
In chapter~\ref{ssec:protclas}, a method of embedding sequences of amino acids has already been discussed. Here, a sequence of amino acids is considered to be a bag of k-mers. Within a k-mer, the amino acids (presented as randomly generated hyperdimensional vectors) are bonded together with sequential information included. All possible k-mers are all then bundled together, the result is then a hyperdimensional vector representing the whole sequence. We also introduce a novel sequence embedding method within the framework of hyperdimensional computing. It is similar to the bag-of-words method in the sense that it bundles vectors of k-mers, but here, the k-mer's positional information will be encoded into the k-mer before bundling. 
%%% picture
\subsection*{Type classification}
Out of the 11549 unambiguous UniParc accessions in the newest version of the database, 4829 are manually annotated on their type. Out of these manually annotated proteins, 2803 are endolysins and 2026 are VALs. As a baseline level, we use the rudimentary HDV classification technique as seen in chapter~\ref{sec:example}: the HDVs of sequences of the same class are bundled to construct single HDVs representative of every class. Then, a sequence's class is inferred by comparing the sequence's HDV to both class HDV \textit{via} a similarity measure based on the assumption that the class vector is maximally similar to its components. This model was evaluated \textit{via} a stratified 10-fold cross validation. The classification with an XGBoost classifier is done via the default XGBoost classifier from \textit{XGBoost.jl v2.2.5} and is evaluated \textit{via} \textit{MLJ.jl v0.19.5} with also a stratified 10-fold cross validation.