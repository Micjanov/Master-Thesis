{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"../src/HDC.jl\")\n",
    "include(\"../src/math.jl\")\n",
    "include(\"../src/experimental.jl\")\n",
    "using MLJ\n",
    "using MLDataPattern\n",
    "using DataFrames\n",
    "using XGBoost\n",
    "using MLJXGBoostInterface\n",
    "using Plots\n",
    "using MultivariateStats\n",
    "using MLJBase\n",
    "using Printf\n",
    "using UMAP\n",
    "using ThreadSafeDicts\n",
    "using MLJ\n",
    "using Printf\n",
    "using CategoricalArrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String, Union{Missing, String}} with 11549 entries:\n",
       "  \"UPI000178DD30\" => \"endolysin\"\n",
       "  \"UPI0013EFDC93\" => \"endolysin\"\n",
       "  \"UPI000172D062\" => \"VAL\"\n",
       "  \"UPI001463ABBB\" => \"endolysin\"\n",
       "  \"UPI000232F56D\" => \"endolysin\"\n",
       "  \"UPI0011625D30\" => \"VAL\"\n",
       "  \"UPI0009882324\" => \"endolysin\"\n",
       "  \"UPI000CA1D611\" => \"VAL\"\n",
       "  \"UPI0006BC2F8A\" => \"endolysin\"\n",
       "  \"UPI000BBF7878\" => \"endolysin\"\n",
       "  \"UPI00138B2696\" => \"endolysin\"\n",
       "  \"UPI00025D6AED\" => \"endolysin\"\n",
       "  \"UPI0010C2D3EE\" => \"endolysin\"\n",
       "  \"UPI000D22144F\" => \"VAL\"\n",
       "  \"UPI0018623B24\" => \"endolysin\"\n",
       "  \"UPI00080F0655\" => \"endolysin\"\n",
       "  \"UPI00022BD3A3\" => \"endolysin\"\n",
       "  \"UPI0008093543\" => \"endolysin\"\n",
       "  \"UPI001463E938\" => \"VAL\"\n",
       "  ⋮               => ⋮"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using JLD\n",
    "phalp_cnn_rand = JLD.load(\"../data/phalp_cnn_rand_real.jld\")[\"embedded_CNN_rand\"]\n",
    "phalp_bow_rand = JLD.load(\"../data/phalp_bow_rand_real.jld\")[\"embedded_bow_rand\"]\n",
    "phalp_bow_esm = JLD.load(\"../data/phalp_bow_esm_real.jld\")[\"phalp_bow_esm\"]\n",
    "phalp_cnn_esm = JLD.load(\"../data/phalp_cnn_esm_real.jld\")[\"embedded_CNN_esm\"]\n",
    "up2type = JLD.load(\"../data/phalp_type.jld\")[\"up2type\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String, Union{Missing, String}} with 2803 entries:\n",
       "  \"UPI000178DD30\" => \"endolysin\"\n",
       "  \"UPI0012B4B15F\" => \"endolysin\"\n",
       "  \"UPI0013EFDC93\" => \"endolysin\"\n",
       "  \"UPI000201BE9F\" => \"endolysin\"\n",
       "  \"UPI000232F56D\" => \"endolysin\"\n",
       "  \"UPI000DF0A1E1\" => \"endolysin\"\n",
       "  \"UPI00138B2696\" => \"endolysin\"\n",
       "  \"UPI000012EA4B\" => \"endolysin\"\n",
       "  \"UPI000178C353\" => \"endolysin\"\n",
       "  \"UPI001436E76F\" => \"endolysin\"\n",
       "  \"UPI000F6BA7D8\" => \"endolysin\"\n",
       "  \"UPI00001A38F6\" => \"endolysin\"\n",
       "  \"UPI00001A9BAB\" => \"endolysin\"\n",
       "  \"UPI001435C02C\" => \"endolysin\"\n",
       "  \"UPI0010B96635\" => \"endolysin\"\n",
       "  \"UPI0015F22725\" => \"endolysin\"\n",
       "  \"UPI000D7DCFA8\" => \"endolysin\"\n",
       "  \"UPI0010B8D78B\" => \"endolysin\"\n",
       "  \"UPI000EB70E47\" => \"endolysin\"\n",
       "  ⋮               => ⋮"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new = filter(x -> (first(x) in collect(keys(phalp_cnn_esm))), up2type)\n",
    "val = filter(x -> (last(x) == \"VAL\"), new)\n",
    "endo = filter(x -> (last(x) == \"endolysin\"), new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stratifiedcvOHD (generic function with 3 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function divide_training_data(train_data, n_splits)\n",
    "    data_splits = []\n",
    "    split_size = length(train_data) ÷ n_splits\n",
    "    for i in 1:n_splits\n",
    "        start_index = (i - 1) * split_size + 1\n",
    "        end_index = i * split_size\n",
    "        push!(data_splits, train_data[start_index:end_index])\n",
    "    end\n",
    "    return data_splits\n",
    "end\n",
    "\n",
    "function stratifiedcvOHD(embeddings, lr = 0.035, k = 10)\n",
    "    Keys = [key for (key, val) in new]\n",
    "    key_seq = [embeddings[i] for i in Keys] # change mebeddings here\n",
    "    Values = [val for (key, val) in new]\n",
    "\n",
    "    lookupind = Dict(zip([i for i in 1:length(key_seq)], Values))\n",
    "\n",
    "    (xtrain, ytrain), (xtest, ytest) = stratifiedobs((key_seq,Values), p = 0.9, shuffle=true)\n",
    "    train_data_splits = divide_training_data([i for i in 1:length(xtrain)], 500)\n",
    "\n",
    "\n",
    "    folf1 = []\n",
    "    ClsV = zeros(10000)\n",
    "    ClsE = zeros(10000)\n",
    "    for i in 1:length(train_data_splits)\n",
    "        x = [xtrain[j] for j in train_data_splits[i]]\n",
    "        y = [ytrain[j] for j in train_data_splits[i]]\n",
    "        \n",
    "        if i == 1\n",
    "            for j in x #init class vectors from random vector\n",
    "                initer = rand([v for v in 1:length(x)])\n",
    "                if ytrain[initer] == \"VAL\"\n",
    "                    ClsV = x[initer]\n",
    "                    deleteat!(x, initer)\n",
    "                    deleteat!(y, initer)\n",
    "                    break\n",
    "                end\n",
    "            end\n",
    "\n",
    "            for j in x #init class vectors from random vector\n",
    "                initer = rand([v for v in 1:length(x)])\n",
    "                if ytrain[initer] == \"endolysin\"\n",
    "                    ClsE = x[initer]\n",
    "                    deleteat!(x, initer)\n",
    "                    deleteat!(y, initer)\n",
    "                    break\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        for j in 1:length(x) #training\n",
    "            realcls = y[j]\n",
    "            seq = x[j]\n",
    "            cosV = cosine(seq, ClsV)\n",
    "            cosE = cosine(seq, ClsE)\n",
    "\n",
    "            if realcls == \"VAL\" && cosV < cosE\n",
    "                ClsV = ClsV .+ (lr*(cosV) .* seq)\n",
    "                ClsE = ClsE .- (lr*(cosE) .* seq)\n",
    "            end\n",
    " \n",
    "            if realcls == \"endolysin\" && cosV > cosE\n",
    "                ClsV = ClsV .- (lr*(cosV) .* seq)\n",
    "                ClsE = ClsE .+ (lr*(cosE) .* seq)\n",
    "            end\n",
    "        end\n",
    "\n",
    "        confusion_matrix = zeros(2,2)\n",
    "\n",
    "        for j in 1:length(xtest) #testing\n",
    "            endo_sim = cosine(xtest[j], ClsE)\n",
    "            val_sim = cosine(xtest[j], ClsV)\n",
    "\n",
    "            if endo_sim > val_sim\n",
    "                if ytest[j] == \"endolysin\"\n",
    "                    confusion_matrix[1,1] += 1\n",
    "                else\n",
    "                    confusion_matrix[2,1] += 1\n",
    "                end\n",
    "            else\n",
    "                if ytest[j] == \"VAL\"\n",
    "                    confusion_matrix[2,2] += 1\n",
    "                else\n",
    "                    confusion_matrix[1,2] += 1\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "\n",
    "        prec = confusion_matrix[1,1]/(confusion_matrix[1,1] + confusion_matrix[1,2])\n",
    "        recall = confusion_matrix[1,1]/(confusion_matrix[1,1] + confusion_matrix[2,1])\n",
    "        f1 = 2 * (prec*recall)/(prec+recall)\n",
    "        push!(folf1, f1)\n",
    "    end\n",
    "    return folf1\n",
    "        \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [(phalp_bow_esm, \"phalp_bow_esm\"), (phalp_cnn_esm ,\"phalp_cnn_esm\"), (phalp_bow_rand, \"phalp_bow_rand\"), (phalp_cnn_rand ,\"phalp_cnn_rand\")]\n",
    "for i in t\n",
    "    f1list = stratifiedcvOHD(i[1])\n",
    "    f1_plot = plot([j for j in 1:length(f1list)], f1list,\n",
    "    xlabel=\"Batch\", ylims = [0.0, 1.0],\n",
    "    ylabel=\"F1-score\", dpi = 300, tickfont=font(14), legend = false, guidefont=font(13))\n",
    "    savefig(@sprintf(\"../thesis/Fig/%s_score.png\", i[2]))\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stratifiedcvOHD (generic function with 3 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function stratifiedcvOHD(embeddings, lr = 0.035)\n",
    "    Keys = [key for (key, val) in new]\n",
    "    key_seq = [embeddings[i] for i in Keys] # change mebeddings here\n",
    "    Values = [val for (key, val) in new]\n",
    "\n",
    "\n",
    "    lookupind = Dict(zip([i for i in 1:length(key_seq)], Values))\n",
    "\n",
    "\n",
    "    follow = []\n",
    "\n",
    "        ClsV = zeros(10000)\n",
    "        ClsE = zeros(10000)\n",
    "\n",
    "        for j in 1:length(key_seq) #init class vectors from random vector\n",
    "            initer = rand(1:length(key_seq))\n",
    "            if Values[initer] == \"VAL\"\n",
    "                ClsV = key_seq[initer]\n",
    "                deleteat!(key_seq, initer)\n",
    "                deleteat!(Values, initer)\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "\n",
    "        for j in 1:length(key_seq) #init class vectors from random vector\n",
    "            initer = rand(1:length(key_seq))\n",
    "            if Values[initer] == \"endolysin\"\n",
    "                ClsE = key_seq[initer]\n",
    "                deleteat!(key_seq, initer)\n",
    "                deleteat!(Values, initer)\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "\n",
    "        for j in 1:length(key_seq) #training\n",
    "            realcls = Values[j]\n",
    "            seq = key_seq[j]\n",
    "            cosV = cosine(seq, ClsV)\n",
    "            cosE = cosine(seq, ClsE)\n",
    "            println(cosE)\n",
    "            if realcls == \"VAL\" && cosV < cosE\n",
    "                ClsV = ClsV .+ (lr*(cosV) .* seq)\n",
    "                ClsE = ClsE .- (lr*(cosE) .* seq)\n",
    "            end\n",
    " \n",
    "            if realcls == \"endolysin\" && cosV > cosE\n",
    "                ClsV = ClsV .- (lr*(cosV) .* seq)\n",
    "                ClsE = ClsE .+ (lr*(cosE) .* seq)\n",
    "            end\n",
    "\n",
    "            push!(follow, abs(cosV-cosE))\n",
    "        end\n",
    "\n",
    "    return follow\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stratifiedcvOHD (generic function with 3 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function stratifiedcvOHD(embeddings, lr = 0.035, k = 10)\n",
    "    Keys = [key for (key, val) in new]\n",
    "    key_seq = [embeddings[i] for i in Keys] # change mebeddings here\n",
    "    Values = [val for (key, val) in new]\n",
    "    println(length(Values))\n",
    "\n",
    "    lookupind = Dict(zip([i for i in 1:length(key_seq)], Values))\n",
    "\n",
    "    stratified_cv = StratifiedCV(;nfolds=k)\n",
    "    pairs = MLJBase.train_test_pairs(stratified_cv, 1:length(Keys), Values)\n",
    "\n",
    "    f1 = 0\n",
    "\n",
    "    followbig = []\n",
    "\n",
    "    big = zeros(2,2)\n",
    "\n",
    "    for i in pairs\n",
    "        train = i[1]\n",
    "        test = i[2]\n",
    "\n",
    "        ClsV = zeros(10000)\n",
    "        ClsE = zeros(10000)\n",
    "\n",
    "        for j in train #init class vectors from random vector\n",
    "            initer = rand(train)\n",
    "            if lookupind[initer] == \"VAL\"\n",
    "                ClsV = key_seq[initer]\n",
    "                filter!(!=(initer), train)\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "\n",
    "        for j in train #init class vectors from random vector\n",
    "            initer = rand(train)\n",
    "            if lookupind[initer] == \"endolysin\"\n",
    "                ClsE = key_seq[initer]\n",
    "                filter!(!=(initer), train)\n",
    "                break\n",
    "            end\n",
    "        end\n",
    "\n",
    "        follow = []\n",
    "\n",
    "        Threads.@threads for j in train #training\n",
    "            realcls = lookupind[j]\n",
    "            seq = key_seq[j]\n",
    "            cosV = cosine(seq, ClsV)\n",
    "            cosE = cosine(seq, ClsE)\n",
    "            println(cos)\n",
    "\n",
    "            if realcls == \"VAL\" && cosV < cosE\n",
    "                ClsV = ClsV .+ (lr*(cosV) .* seq)\n",
    "                ClsE = ClsE .- (lr*(cosE) .* seq)\n",
    "            end\n",
    " \n",
    "            if realcls == \"endolysin\" && cosV > cosE\n",
    "                ClsV = ClsV .- (lr*(cosV) .* seq)\n",
    "                ClsE = ClsE .+ (lr*(cosE) .* seq)\n",
    "            end\n",
    "\n",
    "            push!(follow, abs(cosV-cosE))\n",
    "        end\n",
    "\n",
    "        push!(followbig, follow)\n",
    "\n",
    "        confusion_matrix = zeros(2,2)\n",
    "\n",
    "        Threads.@threads for x in 1:length(test) #testing\n",
    "            endo_sim = cosine(key_seq[test[x]], ClsE)\n",
    "            val_sim = cosine(key_seq[test[x]], ClsV)\n",
    "\n",
    "            if endo_sim > val_sim\n",
    "                if Values[test[x]] == \"endolysin\"\n",
    "                    confusion_matrix[1,1] += 1\n",
    "                else\n",
    "                    confusion_matrix[2,1] += 1\n",
    "                end\n",
    "            else\n",
    "                if Values[test[x]] == \"VAL\"\n",
    "                    confusion_matrix[2,2] += 1\n",
    "                else\n",
    "                    confusion_matrix[1,2] += 1\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "        big = big .+ confusion_matrix\n",
    "        #evaluating\n",
    "        prec = confusion_matrix[1,1]/(confusion_matrix[1,1] + confusion_matrix[1,2])\n",
    "        recall = confusion_matrix[1,1]/(confusion_matrix[1,1] + confusion_matrix[2,1])\n",
    "\n",
    "        f1 += 2 * (prec*recall)/(prec+recall)\n",
    "    end\n",
    "    println(big ./ k)\n",
    "    return f1/k, followbig\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LaTeXStrings\n",
    "t = [(phalp_bow_esm, \"phalp_bow_esm\"), (phalp_cnn_esm ,\"phalp_cnn_esm\"), (phalp_bow_rand, \"phalp_bow_rand\"), (phalp_cnn_rand ,\"phalp_cnn_rand\")]\n",
    "\n",
    "for i in t\n",
    "    # Create a plot of the averages against their indices\n",
    "    plt = plot(1:length(stratifiedcvOHD(i[1])), stratifiedcvOHD(i[1]), markershape=:circle, label = \"\",  seriestype=:scatter, markercolor=:blue, dpi = 300, size = (900, 500), tickfont=font(15), guidefontsize = 15, ylabel = L\"$cos_{VAL} - cos_{endo}$\", xlabel = \"Query vector\", bottom_margin=0.5Plots.cm, left_margin=0.5Plots.cm)\n",
    "\n",
    "    dir = @sprintf(\"../thesis/Fig/%s_learning3.png\", i[2])\n",
    "    savefig(dir)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{cccccccccccccccccccc}\n",
      "$0.0$ & $0.49$ & $0.49$ & $0.49$ & $0.5$ & $0.49$ & $0.48$ & $0.49$ & $0.5$ & $0.49$ & $0.5$ & $0.49$ & $0.5$ & $0.5$ & $0.5$ & $0.49$ & $0.49$ & $0.49$ & $0.49$ & $0.49$\\\\ \\hline\n",
      "$0.49$ & $0.0$ & $0.5$ & $0.5$ & $0.5$ & $0.49$ & $0.49$ & $0.49$ & $0.49$ & $0.49$ & $0.48$ & $0.49$ & $0.48$ & $0.49$ & $0.49$ & $0.49$ & $0.49$ & $0.49$ & $0.49$ & $0.49$\\\\ \\hline\n",
      "$0.49$ & $0.5$ & $0.0$ & $0.49$ & $0.5$ & $0.5$ & $0.5$ & $0.49$ & $0.51$ & $0.48$ & $0.49$ & $0.5$ & $0.49$ & $0.49$ & $0.5$ & $0.5$ & $0.48$ & $0.49$ & $0.49$ & $0.49$\\\\ \\hline\n",
      "$0.49$ & $0.5$ & $0.49$ & $0.0$ & $0.49$ & $0.5$ & $0.49$ & $0.5$ & $0.5$ & $0.5$ & $0.5$ & $0.49$ & $0.49$ & $0.51$ & $0.49$ & $0.49$ & $0.49$ & $0.49$ & $0.48$ & $0.49$\\\\ \\hline\n",
      "$0.5$ & $0.5$ & $0.5$ & $0.49$ & $0.0$ & $0.5$ & $0.5$ & $0.51$ & $0.5$ & $0.5$ & $0.5$ & $0.5$ & $0.5$ & $0.49$ & $0.5$ & $0.49$ & $0.5$ & $0.49$ & $0.5$ & $0.48$\\\\ \\hline\n",
      "$0.49$ & $0.49$ & $0.5$ & $0.5$ & $0.5$ & $0.0$ & $0.49$ & $0.49$ & $0.5$ & $0.5$ & $0.5$ & $0.49$ & $0.49$ & $0.49$ & $0.5$ & $0.49$ & $0.5$ & $0.49$ & $0.49$ & $0.5$\\\\ \\hline\n",
      "$0.48$ & $0.49$ & $0.5$ & $0.49$ & $0.5$ & $0.49$ & $0.0$ & $0.5$ & $0.49$ & $0.49$ & $0.5$ & $0.5$ & $0.5$ & $0.5$ & $0.49$ & $0.5$ & $0.49$ & $0.49$ & $0.49$ & $0.49$\\\\ \\hline\n",
      "$0.49$ & $0.49$ & $0.49$ & $0.5$ & $0.51$ & $0.49$ & $0.5$ & $0.0$ & $0.5$ & $0.5$ & $0.5$ & $0.5$ & $0.5$ & $0.5$ & $0.5$ & $0.5$ & $0.49$ & $0.49$ & $0.49$ & $0.5$\\\\ \\hline\n",
      "$0.5$ & $0.49$ & $0.51$ & $0.5$ & $0.5$ & $0.5$ & $0.49$ & $0.5$ & $0.0$ & $0.49$ & $0.5$ & $0.49$ & $0.49$ & $0.5$ & $0.5$ & $0.49$ & $0.49$ & $0.5$ & $0.49$ & $0.48$\\\\ \\hline\n",
      "$0.49$ & $0.49$ & $0.48$ & $0.5$ & $0.5$ & $0.5$ & $0.49$ & $0.5$ & $0.49$ & $0.0$ & $0.5$ & $0.48$ & $0.49$ & $0.49$ & $0.49$ & $0.5$ & $0.49$ & $0.5$ & $0.48$ & $0.49$\\\\ \\hline\n",
      "$0.5$ & $0.48$ & $0.49$ & $0.5$ & $0.5$ & $0.5$ & $0.5$ & $0.5$ & $0.5$ & $0.5$ & $0.0$ & $0.5$ & $0.5$ & $0.5$ & $0.5$ & $0.49$ & $0.49$ & $0.49$ & $0.49$ & $0.49$\\\\ \\hline\n",
      "$0.49$ & $0.49$ & $0.5$ & $0.49$ & $0.5$ & $0.49$ & $0.5$ & $0.5$ & $0.49$ & $0.48$ & $0.5$ & $0.0$ & $0.51$ & $0.5$ & $0.49$ & $0.49$ & $0.49$ & $0.5$ & $0.49$ & $0.5$\\\\ \\hline\n",
      "$0.5$ & $0.48$ & $0.49$ & $0.49$ & $0.5$ & $0.49$ & $0.5$ & $0.5$ & $0.49$ & $0.49$ & $0.5$ & $0.51$ & $0.0$ & $0.5$ & $0.49$ & $0.49$ & $0.49$ & $0.49$ & $0.49$ & $0.49$\\\\ \\hline\n",
      "$0.5$ & $0.49$ & $0.49$ & $0.51$ & $0.49$ & $0.49$ & $0.5$ & $0.5$ & $0.5$ & $0.49$ & $0.5$ & $0.5$ & $0.5$ & $0.0$ & $0.5$ & $0.5$ & $0.5$ & $0.49$ & $0.5$ & $0.49$\\\\ \\hline\n",
      "$0.5$ & $0.49$ & $0.5$ & $0.49$ & $0.5$ & $0.5$ & $0.49$ & $0.5$ & $0.5$ & $0.49$ & $0.5$ & $0.49$ & $0.49$ & $0.5$ & $0.0$ & $0.5$ & $0.5$ & $0.49$ & $0.49$ & $0.49$\\\\ \\hline\n",
      "$0.49$ & $0.49$ & $0.5$ & $0.49$ & $0.49$ & $0.49$ & $0.5$ & $0.5$ & $0.49$ & $0.5$ & $0.49$ & $0.49$ & $0.49$ & $0.5$ & $0.5$ & $0.0$ & $0.49$ & $0.49$ & $0.49$ & $0.49$\\\\ \\hline\n",
      "$0.49$ & $0.49$ & $0.48$ & $0.49$ & $0.5$ & $0.5$ & $0.49$ & $0.49$ & $0.49$ & $0.49$ & $0.49$ & $0.49$ & $0.49$ & $0.5$ & $0.5$ & $0.49$ & $0.0$ & $0.49$ & $0.49$ & $0.49$\\\\ \\hline\n",
      "$0.49$ & $0.49$ & $0.49$ & $0.49$ & $0.49$ & $0.49$ & $0.49$ & $0.49$ & $0.5$ & $0.5$ & $0.49$ & $0.5$ & $0.49$ & $0.49$ & $0.49$ & $0.49$ & $0.49$ & $0.0$ & $0.49$ & $0.49$\\\\ \\hline\n",
      "$0.49$ & $0.49$ & $0.49$ & $0.48$ & $0.5$ & $0.49$ & $0.49$ & $0.49$ & $0.49$ & $0.48$ & $0.49$ & $0.49$ & $0.49$ & $0.5$ & $0.49$ & $0.49$ & $0.49$ & $0.49$ & $0.0$ & $0.49$\\\\ \\hline\n",
      "$0.49$ & $0.49$ & $0.49$ & $0.49$ & $0.48$ & $0.5$ & $0.49$ & $0.5$ & $0.48$ & $0.49$ & $0.49$ & $0.5$ & $0.49$ & $0.49$ & $0.49$ & $0.49$ & $0.49$ & $0.49$ & $0.49$ & $0.0$\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "using LatexPrint\n",
    "using LinearAlgebra\n",
    "A = [0.0 0.4892 0.4922 0.4863 0.497 0.4933 0.4834 0.4888 0.4962 0.4894 0.4989 0.4924 0.4955 0.4959 0.4971 0.4904 0.4914 0.4877 0.4885 0.4899; 0.4892 0.0 0.4958 0.4963 0.4984 0.4915 0.4876 0.4888 0.4858 0.487 0.4805 0.491 0.4805 0.4925 0.4937 0.4932 0.4868 0.4881 0.4881 0.4949; 0.4922 0.4958 0.0 0.4857 0.5002 0.4997 0.4978 0.4946 0.5078 0.4844 0.4943 0.5 0.4927 0.4945 0.4969 0.497 0.4846 0.4853 0.4867 0.4939; 0.4863 0.4963 0.4857 0.0 0.4903 0.5008 0.4879 0.4989 0.4999 0.4971 0.499 0.4949 0.4894 0.5088 0.4878 0.4889 0.4921 0.4936 0.4824 0.4886; 0.497 0.4984 0.5002 0.4903 0.0 0.5007 0.497 0.509 0.4984 0.5042 0.5011 0.5012 0.4983 0.4929 0.5011 0.49 0.4966 0.4885 0.4955 0.4823; 0.4933 0.4915 0.4997 0.5008 0.5007 0.0 0.4929 0.4941 0.4975 0.4997 0.495 0.4905 0.4898 0.4944 0.5044 0.4917 0.4981 0.487 0.4912 0.4954; 0.4834 0.4876 0.4978 0.4879 0.497 0.4929 0.0 0.4976 0.4898 0.4938 0.4953 0.495 0.5031 0.4997 0.4857 0.5014 0.4904 0.4877 0.4923 0.4921; 0.4888 0.4888 0.4946 0.4989 0.509 0.4941 0.4976 0.0 0.499 0.4964 0.5043 0.4966 0.5021 0.5001 0.5029 0.5002 0.4928 0.4893 0.4891 0.5019; 0.4962 0.4858 0.5078 0.4999 0.4984 0.4975 0.4898 0.499 0.0 0.493 0.4965 0.492 0.4949 0.4983 0.4995 0.4946 0.4872 0.4973 0.4869 0.4819; 0.4894 0.487 0.4844 0.4971 0.5042 0.4997 0.4938 0.4964 0.493 0.0 0.4971 0.485 0.4931 0.4941 0.4877 0.4986 0.492 0.4973 0.4847 0.4889; 0.4989 0.4805 0.4943 0.499 0.5011 0.495 0.4953 0.5043 0.4965 0.4971 0.0 0.4981 0.5002 0.4972 0.496 0.4911 0.4939 0.4932 0.4932 0.4898; 0.4924 0.491 0.5 0.4949 0.5012 0.4905 0.495 0.4966 0.492 0.485 0.4981 0.0 0.5055 0.4969 0.4933 0.493 0.4912 0.4957 0.4949 0.4955; 0.4955 0.4805 0.4927 0.4894 0.4983 0.4898 0.5031 0.5021 0.4949 0.4931 0.5002 0.5055 0.0 0.4972 0.4932 0.4931 0.4919 0.4924 0.4864 0.489; 0.4959 0.4925 0.4945 0.5088 0.4929 0.4944 0.4997 0.5001 0.4983 0.4941 0.4972 0.4969 0.4972 0.0 0.4958 0.5009 0.5003 0.4874 0.4952 0.4854; 0.4971 0.4937 0.4969 0.4878 0.5011 0.5044 0.4857 0.5029 0.4995 0.4877 0.496 0.4933 0.4932 0.4958 0.0 0.5019 0.4963 0.4866 0.4946 0.4872; 0.4904 0.4932 0.497 0.4889 0.49 0.4917 0.5014 0.5002 0.4946 0.4986 0.4911 0.493 0.4931 0.5009 0.5019 0.0 0.4922 0.4949 0.4925 0.4929; 0.4914 0.4868 0.4846 0.4921 0.4966 0.4981 0.4904 0.4928 0.4872 0.492 0.4939 0.4912 0.4919 0.5003 0.4963 0.4922 0.0 0.4915 0.4859 0.4863; 0.4877 0.4881 0.4853 0.4936 0.4885 0.487 0.4877 0.4893 0.4973 0.4973 0.4932 0.4957 0.4924 0.4874 0.4866 0.4949 0.4915 0.0 0.486 0.4886; 0.4885 0.4881 0.4867 0.4824 0.4955 0.4912 0.4923 0.4891 0.4869 0.4847 0.4932 0.4949 0.4864 0.4952 0.4946 0.4925 0.4859 0.486 0.0 0.4942; 0.4899 0.4949 0.4939 0.4886 0.4823 0.4954 0.4921 0.5019 0.4819 0.4889 0.4898 0.4955 0.489 0.4854 0.4872 0.4929 0.4863 0.4886 0.4942 0.0]\n",
    "tabular(round.(A, digits = 2), hlines=true)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.0",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
