{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cosine"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Random\n",
    "using LinearAlgebra\n",
    "\"\"\"\n",
    "Construct a binary vector. By default 10000 elements long.\n",
    "\"\"\"\n",
    "hdv(N::Int=10000) = rand((-1,1), 1, N)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Bundles bipolar hyperdimensional vectors.\n",
    "\"\"\"\n",
    "add(vectors::Vector{Int}...) = reduce(.+, vectors) .|> sign\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Binds binpolar hyperdimensional vectors.\n",
    "\"\"\"\n",
    "multiply(vectors::Vector{Int}...) = reduce(.*, vectors)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Permutes a bipolar hyperdimensional vector by an adjustable circular shift.\n",
    "\"\"\"\n",
    "perm(vector::Vector, k::Int=1) = circshift(vector, (0, k))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Calculates the cosine similarity between two bipolar vectors.\n",
    "\"\"\"\n",
    "cosine(x, y) = dot(x, y) / (norm(x) * norm(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hamming"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Construct a binary vector. By default 10000 elements long.\n",
    "\"\"\"\n",
    "bithdv(N::Int=10000) = bitrand(N)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Bundles binary hyperdimensional vectors based on the element-wise majority rule.\n",
    "\"\"\"\n",
    "function bitadd(vectors::BitVector ...)\n",
    "    v = reduce(.+, vectors)\n",
    "    n = length(vectors) / 2\n",
    "    x = [i > n ? 1 : i < n ? 0 : rand(0:1) for i in v]\n",
    "    return convert(BitVector, x)\n",
    "end\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Binds binary hyperdimensional vectors based on an element-wise XOR gate.\n",
    "\"\"\"\n",
    "bitbind(vectors::BitVector ...) =  reduce(.⊻, vectors)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Permutes a binary hyperdimensional vector by an adjustable circular shift.\n",
    "\"\"\"\n",
    "bitperm(vector::BitVector, k::Int=1) = circshift(vector, k)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Calculates the Hamming distance between two binary vectors.\n",
    "\"\"\"\n",
    "hamming(x::BitVector, y::BitVector) = sum(x .!= y)/length(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"data-frame\"><p>5 rows × 4 columns</p><table class=\"data-frame\"><thead><tr><th></th><th>ID</th><th>sequence</th><th>class</th><th>class_num</th></tr><tr><th></th><th title=\"Int64\">Int64</th><th title=\"String\">String</th><th title=\"String31\">String31</th><th title=\"Int64\">Int64</th></tr></thead><tbody><tr><th>1</th><td>1</td><td>AAWKWAWAKKWAKAKKWAKAA</td><td>mod. active</td><td>2</td></tr><tr><th>2</th><td>2</td><td>AIGKFLHSAKKFGKAFVGEIMNS</td><td>mod. active</td><td>2</td></tr><tr><th>3</th><td>3</td><td>AWKKWAKAWKWAKAKWWAKAA</td><td>mod. active</td><td>2</td></tr><tr><th>4</th><td>4</td><td>ESFSDWWKLLAE</td><td>mod. active</td><td>2</td></tr><tr><th>5</th><td>5</td><td>ETFADWWKLLAE</td><td>mod. active</td><td>2</td></tr></tbody></table></div>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccc}\n",
       "\t& ID & sequence & class & class\\_num\\\\\n",
       "\t\\hline\n",
       "\t& Int64 & String & String31 & Int64\\\\\n",
       "\t\\hline\n",
       "\t1 & 1 & AAWKWAWAKKWAKAKKWAKAA & mod. active & 2 \\\\\n",
       "\t2 & 2 & AIGKFLHSAKKFGKAFVGEIMNS & mod. active & 2 \\\\\n",
       "\t3 & 3 & AWKKWAKAWKWAKAKWWAKAA & mod. active & 2 \\\\\n",
       "\t4 & 4 & ESFSDWWKLLAE & mod. active & 2 \\\\\n",
       "\t5 & 5 & ETFADWWKLLAE & mod. active & 2 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "\u001b[1m5×4 DataFrame\u001b[0m\n",
       "\u001b[1m Row \u001b[0m│\u001b[1m ID    \u001b[0m\u001b[1m sequence                \u001b[0m\u001b[1m class       \u001b[0m\u001b[1m class_num \u001b[0m\n",
       "\u001b[1m     \u001b[0m│\u001b[90m Int64 \u001b[0m\u001b[90m String                  \u001b[0m\u001b[90m String31    \u001b[0m\u001b[90m Int64     \u001b[0m\n",
       "─────┼────────────────────────────────────────────────────────\n",
       "   1 │     1  AAWKWAWAKKWAKAKKWAKAA    mod. active          2\n",
       "   2 │     2  AIGKFLHSAKKFGKAFVGEIMNS  mod. active          2\n",
       "   3 │     3  AWKKWAKAWKWAKAKWWAKAA    mod. active          2\n",
       "   4 │     4  ESFSDWWKLLAE             mod. active          2\n",
       "   5 │     5  ETFADWWKLLAE             mod. active          2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using DataFrames, CSV\n",
    "data = CSV.read(\"ProtExdata/ACPs_Breast_cancer.csv\", DataFrame)\n",
    "unique(data.class)\n",
    "class_num = [i == \"very active\" ? 1 : i == \"mod. active\" ? 2 : i == \"inactive - exp\" ? 3 : 4 for i in data.class]\n",
    "data[!, :class_num] = class_num\n",
    "data = data[data.class_num .!= 4, :]\n",
    "first(data, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1828955   0.5040589   0.39757144 ...  0.9225599   0.05342245\n",
      "  -0.45949394]\n",
      " [ 0.3699865   0.28748336 -0.0116293  ...  0.12235592  0.17939556\n",
      "  -0.2283884 ]\n",
      " [ 0.08383    -0.2629382   0.2560887  ... -0.10995694  0.3136453\n",
      "   0.14911757]\n",
      " ...\n",
      " [ 0.2191191  -0.28030506  0.46562314 ... -0.02370527  0.45409355\n",
      "  -0.10943315]\n",
      " [ 0.25577268 -0.34152842  0.41236705 ...  0.0049744   0.4067464\n",
      "  -0.07702404]\n",
      " [ 0.11428512 -0.08154328  0.43711823 ...  0.24650113 -0.06088951\n",
      "  -0.1885616 ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/esm2_t6_8M_UR50D were not used when initializing EsmModel: ['esm.contact_head.regression.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'esm.contact_head.regression.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing EsmModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing EsmModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['esm.pooler.dense.weight', 'esm.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "using PyCall\n",
    "\n",
    "py\"\"\"\n",
    "import numpy as np\n",
    "from transformers import EsmModel, EsmConfig, EsmTokenizer\n",
    "import torch\n",
    "\n",
    "embeddings = \"facebook/esm2_t6_8M_UR50D\"\n",
    "tokenizer = EsmTokenizer.from_pretrained(embeddings)\n",
    "model = EsmModel.from_pretrained(embeddings)\n",
    "\n",
    "seq = 'ARNDCQEGHILKMFPSTWYVOUBJZX'\n",
    "inputs = tokenizer(seq, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "last_hidden_states = outputs.last_hidden_state[0].detach().numpy()\n",
    "print(last_hidden_states)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Char, BitVector} with 26 entries:\n",
       "  'E' => [0, 1, 0, 1, 1, 0, 1, 1, 0, 1  …  1, 1, 0, 1, 0, 1, 1, 1, 0, 1]\n",
       "  'Z' => [0, 1, 0, 1, 1, 0, 0, 1, 0, 0  …  1, 0, 0, 0, 1, 0, 1, 0, 1, 1]\n",
       "  'X' => [0, 1, 1, 1, 1, 0, 0, 1, 0, 0  …  1, 0, 0, 0, 1, 0, 1, 0, 1, 1]\n",
       "  'C' => [1, 1, 1, 0, 1, 1, 1, 1, 0, 1  …  0, 0, 1, 1, 0, 1, 0, 0, 0, 0]\n",
       "  'B' => [0, 0, 1, 1, 1, 1, 0, 1, 0, 0  …  1, 0, 0, 0, 1, 0, 1, 0, 1, 1]\n",
       "  'D' => [0, 0, 0, 0, 0, 1, 0, 0, 0, 0  …  1, 1, 1, 1, 1, 0, 1, 0, 0, 1]\n",
       "  'A' => [0, 1, 0, 0, 1, 1, 1, 0, 1, 1  …  1, 1, 1, 0, 0, 1, 0, 1, 1, 0]\n",
       "  'R' => [0, 0, 0, 1, 1, 1, 0, 1, 1, 0  …  1, 0, 1, 0, 0, 0, 0, 1, 0, 1]\n",
       "  'G' => [0, 1, 0, 0, 1, 1, 0, 1, 1, 1  …  1, 1, 0, 0, 0, 0, 1, 0, 0, 1]\n",
       "  'N' => [1, 0, 0, 1, 0, 0, 1, 0, 0, 1  …  1, 1, 1, 0, 0, 1, 1, 0, 0, 1]\n",
       "  'Q' => [1, 0, 0, 0, 0, 1, 0, 0, 1, 0  …  0, 0, 1, 1, 0, 1, 1, 1, 0, 1]\n",
       "  'M' => [1, 0, 0, 0, 1, 0, 1, 1, 0, 0  …  1, 1, 1, 1, 1, 0, 1, 0, 0, 0]\n",
       "  'K' => [0, 0, 0, 1, 0, 0, 0, 1, 0, 0  …  0, 0, 1, 0, 1, 1, 0, 0, 1, 1]\n",
       "  'F' => [0, 1, 0, 1, 1, 1, 1, 1, 0, 0  …  1, 1, 1, 1, 0, 1, 1, 0, 0, 1]\n",
       "  'P' => [0, 0, 1, 0, 0, 0, 0, 1, 0, 0  …  0, 0, 1, 1, 1, 1, 1, 0, 1, 0]\n",
       "  'I' => [0, 0, 1, 1, 1, 1, 1, 1, 1, 1  …  0, 0, 0, 1, 0, 0, 0, 0, 0, 1]\n",
       "  'O' => [0, 0, 0, 1, 1, 1, 0, 1, 0, 0  …  1, 0, 0, 0, 1, 0, 1, 0, 1, 1]\n",
       "  'H' => [1, 0, 0, 0, 1, 1, 0, 0, 0, 0  …  1, 0, 1, 0, 0, 0, 0, 0, 0, 1]\n",
       "  'J' => [0, 1, 1, 0, 1, 0, 0, 1, 0, 0  …  0, 0, 0, 0, 1, 0, 1, 0, 0, 1]\n",
       "  'W' => [0, 1, 1, 0, 0, 1, 0, 1, 0, 0  …  1, 0, 1, 0, 1, 1, 0, 0, 1, 0]\n",
       "  'S' => [0, 0, 0, 1, 1, 0, 0, 1, 1, 0  …  0, 0, 1, 1, 0, 1, 1, 1, 0, 1]\n",
       "  'T' => [1, 1, 1, 1, 1, 0, 0, 0, 1, 0  …  0, 0, 1, 0, 0, 0, 0, 1, 0, 0]\n",
       "  'U' => [0, 0, 1, 1, 1, 1, 0, 1, 0, 0  …  1, 0, 0, 0, 1, 0, 1, 0, 1, 1]\n",
       "  'L' => [0, 0, 0, 1, 0, 0, 1, 0, 0, 0  …  0, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n",
       "  'Y' => [0, 1, 1, 1, 1, 1, 1, 0, 1, 1  …  0, 0, 1, 0, 1, 1, 1, 1, 1, 0]\n",
       "  'V' => [0, 0, 1, 0, 1, 0, 0, 1, 1, 1  …  0, 1, 1, 1, 1, 1, 0, 0, 0, 1]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using StatsBase\n",
    "\n",
    "AA_list = ['A', 'R', 'N', 'D', 'C', 'Q', 'E', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V', 'O', 'U', 'B', 'J', 'Z', 'X']\n",
    "embeddings_esm = PyArray(py\"last_hidden_states\"o)[2:27, :]\n",
    "\n",
    "dt = fit(UnitRangeTransform, embeddings_esm, dims = 1)\n",
    "embeddings_esm = StatsBase.transform(dt, embeddings_esm)\n",
    "\n",
    "random_hdv = permutedims(hcat([bithdv() for i in 1:320]...))\n",
    "\n",
    "embeddings_esm_hdv = embeddings_esm*random_hdv\n",
    "dt = fit(UnitRangeTransform, embeddings_esm_hdv, dims = 1)\n",
    "embeddings_esm_hdv = round.(StatsBase.transform(dt, embeddings_esm_hdv))\n",
    "\n",
    "hdvs = [convert(BitVector, embeddings_esm_hdv[i, :]) for i in 1:26]\n",
    "AA_dict_esm = Dict(zip(AA_list, hdvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EZ\n",
      "0.9116949\n",
      "0.4626\n",
      "AG\n",
      "0.9950109\n",
      "0.5103\n",
      "AP\n",
      "0.8230575\n",
      "0.5444\n",
      "HK\n",
      "0.985411\n",
      "0.4482\n"
     ]
    }
   ],
   "source": [
    "println(AA_list[7], AA_list[25])\n",
    "println(cosine(embeddings_esm[7,:], embeddings_esm[25,:]))\n",
    "println(hamming(AA_dict_esm['E'], AA_dict_esm['Z']))\n",
    "\n",
    "println(AA_list[1], AA_list[8])\n",
    "println(cosine(embeddings_esm[1,:], embeddings_esm[8,:]))\n",
    "println(hamming(AA_dict_esm['A'], AA_dict_esm['G']))\n",
    "\n",
    "println(AA_list[1], AA_list[15])\n",
    "println(cosine(embeddings_esm[1,:], embeddings_esm[15,:]))\n",
    "println(hamming(AA_dict_esm['A'], AA_dict_esm['P']))\n",
    "\n",
    "println(AA_list[9], AA_list[12])\n",
    "println(cosine(embeddings_esm[9,:], embeddings_esm[12,:]))\n",
    "println(hamming(AA_dict_esm['H'], AA_dict_esm['K']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Char, BitVector} with 26 entries:\n",
       "  'E' => [0, 1, 1, 0, 0, 0, 1, 1, 1, 0  …  0, 0, 1, 1, 1, 0, 0, 0, 0, 1]\n",
       "  'Z' => [1, 1, 1, 0, 0, 1, 1, 1, 0, 1  …  0, 1, 1, 0, 1, 1, 1, 1, 0, 1]\n",
       "  'X' => [1, 1, 1, 0, 0, 1, 1, 1, 0, 1  …  0, 1, 1, 0, 1, 1, 1, 1, 0, 1]\n",
       "  'C' => [0, 0, 1, 1, 0, 0, 0, 0, 1, 0  …  0, 0, 1, 1, 0, 1, 0, 0, 1, 1]\n",
       "  'B' => [1, 1, 1, 0, 0, 1, 1, 1, 0, 1  …  0, 1, 1, 0, 1, 1, 1, 1, 0, 1]\n",
       "  'D' => [0, 1, 1, 0, 0, 1, 1, 1, 1, 1  …  0, 1, 1, 0, 1, 0, 1, 1, 0, 1]\n",
       "  'A' => [0, 1, 1, 0, 0, 1, 1, 1, 1, 1  …  0, 1, 1, 0, 1, 0, 1, 1, 0, 0]\n",
       "  'R' => [0, 1, 1, 0, 0, 0, 1, 1, 1, 1  …  0, 1, 1, 1, 1, 1, 1, 1, 0, 1]\n",
       "  'G' => [0, 1, 1, 0, 0, 1, 1, 1, 1, 1  …  0, 1, 1, 0, 1, 0, 1, 1, 0, 1]\n",
       "  'N' => [0, 1, 1, 0, 0, 1, 1, 1, 1, 1  …  0, 1, 1, 0, 1, 0, 1, 1, 0, 1]\n",
       "  'Q' => [1, 1, 1, 0, 0, 1, 1, 1, 1, 1  …  0, 1, 1, 0, 1, 0, 1, 1, 0, 1]\n",
       "  'M' => [1, 1, 1, 0, 0, 1, 1, 1, 1, 1  …  0, 1, 1, 0, 1, 1, 1, 1, 0, 1]\n",
       "  'K' => [1, 1, 1, 0, 0, 1, 1, 1, 1, 1  …  0, 1, 1, 0, 1, 0, 1, 1, 0, 1]\n",
       "  'F' => [0, 1, 1, 0, 0, 0, 0, 1, 1, 0  …  0, 0, 1, 1, 0, 0, 0, 0, 0, 1]\n",
       "  'P' => [0, 1, 1, 0, 0, 0, 1, 1, 1, 0  …  0, 1, 1, 1, 1, 1, 0, 0, 0, 1]\n",
       "  'I' => [0, 1, 1, 0, 0, 0, 1, 1, 1, 1  …  0, 1, 1, 1, 1, 1, 1, 1, 0, 1]\n",
       "  'O' => [1, 1, 1, 0, 0, 1, 1, 1, 0, 1  …  0, 1, 1, 0, 1, 1, 1, 1, 0, 1]\n",
       "  'H' => [0, 1, 1, 0, 0, 1, 1, 1, 1, 1  …  0, 1, 1, 0, 1, 0, 1, 1, 0, 1]\n",
       "  'J' => [1, 1, 1, 0, 0, 1, 1, 1, 0, 1  …  0, 1, 1, 0, 1, 0, 1, 1, 0, 0]\n",
       "  'W' => [0, 1, 1, 0, 0, 0, 1, 1, 1, 1  …  0, 1, 1, 1, 1, 1, 0, 0, 0, 1]\n",
       "  'S' => [1, 1, 1, 0, 0, 1, 1, 1, 1, 1  …  0, 1, 1, 0, 1, 0, 1, 1, 0, 0]\n",
       "  'T' => [1, 1, 1, 0, 0, 1, 1, 1, 1, 1  …  0, 1, 1, 0, 1, 0, 1, 1, 0, 0]\n",
       "  'U' => [1, 1, 1, 0, 0, 1, 1, 1, 0, 1  …  0, 1, 1, 0, 1, 1, 1, 1, 0, 1]\n",
       "  'L' => [1, 0, 0, 1, 1, 1, 0, 0, 0, 0  …  1, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
       "  'Y' => [0, 0, 1, 0, 0, 0, 0, 1, 1, 0  …  0, 0, 1, 1, 0, 1, 0, 0, 0, 0]\n",
       "  'V' => [1, 0, 0, 1, 1, 1, 0, 0, 0, 0  …  1, 0, 0, 0, 0, 1, 0, 0, 1, 0]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using PyCall\n",
    "using StatsBase\n",
    "py\"\"\"\n",
    "import pickle\n",
    "infile = open(\"/home/mfat/Downloads/data(2).pkl\",'rb')\n",
    "embeddings_h = pickle.load(infile)\n",
    "infile.close()\n",
    "\"\"\"\n",
    "\n",
    "embeddings_trans = PyArray(py\"embeddings_h \"o)\n",
    "dt = fit(UnitRangeTransform, embeddings_trans, dims = 1)\n",
    "embeddings_esm = StatsBase.transform(dt, embeddings_trans)\n",
    "\n",
    "random_hdv = permutedims(hcat([bithdv() for i in 1:1024]...))\n",
    "\n",
    "embeddings_trans_hdv = embeddings_trans*random_hdv\n",
    "dt = fit(UnitRangeTransform, embeddings_trans_hdv, dims = 1)\n",
    "embeddings_trans_hdv = round.(StatsBase.transform(dt, embeddings_trans_hdv))\n",
    "\n",
    "hdvs = [convert(BitVector, embeddings_trans_hdv[i, :]) for i in 1:26]\n",
    "AA_dict_trans = Dict(zip(AA_list, hdvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EZ\n",
      "0.99999964\n",
      "0.3136\n",
      "AG\n",
      "0.9999999\n",
      "0.0238\n",
      "AP\n",
      "0.99999905\n",
      "0.3048\n",
      "HK\n",
      "1.0\n",
      "0.0579\n"
     ]
    }
   ],
   "source": [
    "println(AA_list[7], AA_list[25])\n",
    "println(cosine(embeddings_trans[7,:], embeddings_trans[25,:]))\n",
    "println(hamming(AA_dict_trans['E'], AA_dict_trans['Z']))\n",
    "\n",
    "println(AA_list[1], AA_list[8])\n",
    "println(cosine(embeddings_trans[1,:], embeddings_trans[8,:]))\n",
    "println(hamming(AA_dict_trans['A'], AA_dict_trans['G']))\n",
    "\n",
    "println(AA_list[1], AA_list[15])\n",
    "println(cosine(embeddings_trans[1,:], embeddings_trans[15,:]))\n",
    "println(hamming(AA_dict_trans['A'], AA_dict_trans['P']))\n",
    "\n",
    "println(AA_list[9], AA_list[12])\n",
    "println(cosine(embeddings_trans[9,:], embeddings_trans[12,:]))\n",
    "println(hamming(AA_dict_trans['H'], AA_dict_trans['K']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function convolved_embedding(sequence, tokens, k=3)\n",
    "    \"\"\"\n",
    "    Simple 2-layer convolved embedding in hyperdimensional space\n",
    "    \"\"\"\n",
    "    # layer 1\n",
    "    kmer_hdvs = []\n",
    "    for i in 1:length(sequence)-k+1\n",
    "        kmer = sequence[i:i+k-1]\n",
    "        aa_hdvs = [circshift(tokens[aa], k-l) for (l, aa) in enumerate(kmer)]\n",
    "        push!(kmer_hdvs, bitbind(aa_hdvs))\n",
    "    end\n",
    "    \n",
    "    # layer 2\n",
    "    conv_kmer_hdvs = []\n",
    "    for i in 1:length(kmer_hdvs)-k+1\n",
    "        convolved_kmers = kmer_hdvs[i:i+k-1]\n",
    "        conv_hdvs = [circshift(convolved_kmers[l], k-l) for (l, km) in enumerate(convolved_kmers)]\n",
    "        push!(conv_kmer_hdvs, bitbind(conv_hdvs))\n",
    "    end\n",
    "    \n",
    "    return bitadd(conv_kmer_hdvs)\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
