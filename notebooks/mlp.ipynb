{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using MLJBase.accuracy in module Main conflicts with an existing identifier.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nested_arrays2mat (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using Printf\n",
    "using DataFrames\n",
    "using CSV\n",
    "using ProgressMeter\n",
    "using JLD\n",
    "using ThreadSafeDicts\n",
    "using Flux\n",
    "using MLJ\n",
    "using Random\n",
    "using LinearAlgebra\n",
    "using MLJBase\n",
    "using MLDataPattern\n",
    "using Flux.Data: DataLoader\n",
    "using Flux: onehotbatch, onecold, logitcrossentropy, throttle, @epochs, params\n",
    "using Base.Iterators: repeated\n",
    "\n",
    "bithdv(N::Int=10000) = bitrand(N)\n",
    "\n",
    "function bitadd(vectors::BitVector ...)\n",
    "    v = reduce(.+, vectors)\n",
    "    n = length(vectors) / 2\n",
    "    x = [i > n ? 1 : i < n ? 0 : rand(0:1) for i in v]\n",
    "    return convert(BitVector, x)\n",
    "end\n",
    "\n",
    "bitbind(vectors::BitVector ...) =  reduce(.⊻, vectors)\n",
    "\n",
    "bitperm(vector::BitVector, k::Int=1) = circshift(vector, k)\n",
    "\n",
    "hamming(x::BitVector, y::BitVector) = sum(x .!= y)/length(x)\n",
    "\n",
    "function scaler(row, lower, upper)\n",
    "    minx = minimum(row)\n",
    "    maxx = maximum(row)\n",
    "    x = [lower + ((i - minx)*(upper-lower))/(maxx - minx) for i in row]\n",
    "    return x\n",
    "end\n",
    "\n",
    "function mat_scaler(matrix, lower, upper, dim = 1)\n",
    "    if dim == 2\n",
    "        scaled = reduce(hcat, [scaler(matrix[:, i], lower, upper) for i in 1:size(matrix, 2)])\n",
    "    elseif dim == 1\n",
    "        scaled = permutedims(hcat([scaler(matrix[i, :], lower, upper) for i in 1:size(matrix, 1)]...))\n",
    "    end\n",
    "    return scaled\n",
    "end\n",
    "\n",
    "function nested_arrays2mat(arrays, pd = false)\n",
    "    if pd == false\n",
    "        return reduce(hcat,arrays)\n",
    "    else\n",
    "        return permutedims(reduce(hcat,arrays))\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start loading\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "801"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "netsurf = CSV.read(\"../data/netsurf.csv\", DataFrame) # read dataset\n",
    "select!(netsurf, Not(\" cb513_mask\")) # remove column\n",
    "for i in [\"input\", \" dssp3\", \" dssp8\"] # remove white spaces in strings\n",
    "    netsurf[!, i] = [join(map(x -> isspace(a[x]) ? \"\" : a[x], 1:length(a))) for a in netsurf[!, i]]\n",
    "end\n",
    "\n",
    "seq_list = netsurf.input # all sequences\n",
    "AA_set = union(hcat([Set(i) for i in seq_list])...) # get all possible amino acids\n",
    "\n",
    "function loader()\n",
    "    x = vcat(\n",
    "    JLD.load(\"../data/k250.jld\")[\"k25[ranges[i]:ranges[i + 1]]\"], \n",
    "    [JLD.load(@sprintf(\"../data/k25%s.jld\", i))[\"k25[ranges[i] + 1:ranges[i + 1]]\"] for i in 1:3]...,)\n",
    "    return x\n",
    "end\n",
    "println(\"start loading\")\n",
    "k25 = loader()\n",
    "println(\"loaded\")\n",
    "\n",
    "n_seq = length(k25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "getdata (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function getdata(args)\n",
    "    x_data = [x for i in k25 for x in i] # set x and y arrays\n",
    "    xl = length(x_data)\n",
    "    y_data = first([x for i in netsurf[!, \" dssp8\"] for x in i], xl)\n",
    "    \n",
    "    (X_train,y_train), (X_test,y_test) = stratifiedobs((x_data,y_data), p = 0.7, shuffle = true)\n",
    "    X_train = hcat(X_train...)\n",
    "    X_test = hcat(X_test...)\n",
    "    \n",
    "    struct_set = union(hcat([Set(i) for i in netsurf[!, \" dssp8\"]])...)\n",
    "    y_train, y_test = onehotbatch(y_train, struct_set), onehotbatch(y_test, struct_set)\n",
    "\n",
    "    train_data = DataLoader((X_train, y_train), batchsize = args.batchsize, shuffle=true)\n",
    "    test_data = DataLoader((X_test, y_test), batchsize = args.batchsize)\n",
    "    \n",
    "    return train_data, test_data\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "build_model (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function loss_all(dataloader, model)\n",
    "    l = 0f0\n",
    "    for (x,y) in dataloader\n",
    "        l += logitcrossentropy(model(x), y)\n",
    "    end\n",
    "    l/length(dataloader)\n",
    "end\n",
    "\n",
    "function accuracy(data_loader, model)\n",
    "    acc = 0\n",
    "    for (x,y) in data_loader\n",
    "        acc += sum(onecold(cpu(model(x))) .== onecold(cpu(y)))*1 / size(x,2)\n",
    "    end\n",
    "    acc/length(data_loader)\n",
    "end\n",
    "loss(x,y) = logitcrossentropy(m(x), y)\n",
    "\n",
    "function build_model(; size=10000, nclasses=8)\n",
    "    return Chain(\n",
    " \t    Dense(prod(size), 32, relu),\n",
    "            Dense(32, nclasses))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Args"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Base.@kwdef mutable struct Args\n",
    "    rate::Float64 = 3e-4    # learning rate\n",
    "    batchsize::Int = 1024   # batch size\n",
    "    epochs::Int = 10      # number of epochs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "801-element Vector{Char}:\n",
       " 'C': ASCII/Unicode U+0043 (category Lu: Letter, uppercase)\n",
       " 'C': ASCII/Unicode U+0043 (category Lu: Letter, uppercase)\n",
       " 'C': ASCII/Unicode U+0043 (category Lu: Letter, uppercase)\n",
       " 'C': ASCII/Unicode U+0043 (category Lu: Letter, uppercase)\n",
       " 'H': ASCII/Unicode U+0048 (category Lu: Letter, uppercase)\n",
       " 'H': ASCII/Unicode U+0048 (category Lu: Letter, uppercase)\n",
       " 'H': ASCII/Unicode U+0048 (category Lu: Letter, uppercase)\n",
       " 'H': ASCII/Unicode U+0048 (category Lu: Letter, uppercase)\n",
       " 'H': ASCII/Unicode U+0048 (category Lu: Letter, uppercase)\n",
       " 'H': ASCII/Unicode U+0048 (category Lu: Letter, uppercase)\n",
       " ⋮\n",
       " 'S': ASCII/Unicode U+0053 (category Lu: Letter, uppercase)\n",
       " 'T': ASCII/Unicode U+0054 (category Lu: Letter, uppercase)\n",
       " 'T': ASCII/Unicode U+0054 (category Lu: Letter, uppercase)\n",
       " 'G': ASCII/Unicode U+0047 (category Lu: Letter, uppercase)\n",
       " 'G': ASCII/Unicode U+0047 (category Lu: Letter, uppercase)\n",
       " 'G': ASCII/Unicode U+0047 (category Lu: Letter, uppercase)\n",
       " 'S': ASCII/Unicode U+0053 (category Lu: Letter, uppercase)\n",
       " 'C': ASCII/Unicode U+0043 (category Lu: Letter, uppercase)\n",
       " 'E': ASCII/Unicode U+0045 (category Lu: Letter, uppercase)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196781-element Vector{Char}:\n",
       " 'C': ASCII/Unicode U+0043 (category Lu: Letter, uppercase)\n",
       " 'C': ASCII/Unicode U+0043 (category Lu: Letter, uppercase)\n",
       " 'C': ASCII/Unicode U+0043 (category Lu: Letter, uppercase)\n",
       " 'C': ASCII/Unicode U+0043 (category Lu: Letter, uppercase)\n",
       " 'H': ASCII/Unicode U+0048 (category Lu: Letter, uppercase)\n",
       " 'H': ASCII/Unicode U+0048 (category Lu: Letter, uppercase)\n",
       " 'H': ASCII/Unicode U+0048 (category Lu: Letter, uppercase)\n",
       " 'H': ASCII/Unicode U+0048 (category Lu: Letter, uppercase)\n",
       " 'H': ASCII/Unicode U+0048 (category Lu: Letter, uppercase)\n",
       " 'H': ASCII/Unicode U+0048 (category Lu: Letter, uppercase)\n",
       " ⋮\n",
       " 'C': ASCII/Unicode U+0043 (category Lu: Letter, uppercase)\n",
       " 'C': ASCII/Unicode U+0043 (category Lu: Letter, uppercase)\n",
       " 'C': ASCII/Unicode U+0043 (category Lu: Letter, uppercase)\n",
       " 'C': ASCII/Unicode U+0043 (category Lu: Letter, uppercase)\n",
       " 'C': ASCII/Unicode U+0043 (category Lu: Letter, uppercase)\n",
       " 'C': ASCII/Unicode U+0043 (category Lu: Letter, uppercase)\n",
       " 'C': ASCII/Unicode U+0043 (category Lu: Letter, uppercase)\n",
       " 'C': ASCII/Unicode U+0043 (category Lu: Letter, uppercase)\n",
       " 'C': ASCII/Unicode U+0043 (category Lu: Letter, uppercase)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_data = [x for i in k25 for x in i] # set x and y arrays\n",
    "xl = length(x_data)\n",
    "y_data = first([x for i in netsurf[!, \" dssp8\"] for x in i], xl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196781-element Vector{BitVector}:\n",
       " [1, 1, 0, 0, 0, 0, 0, 0, 1, 0  …  1, 1, 0, 1, 1, 0, 0, 0, 0, 0]\n",
       " [0, 0, 1, 0, 0, 0, 0, 1, 0, 1  …  1, 0, 0, 0, 0, 0, 1, 1, 0, 0]\n",
       " [0, 1, 0, 1, 1, 1, 0, 0, 0, 0  …  1, 0, 1, 0, 0, 0, 0, 1, 0, 1]\n",
       " [1, 1, 0, 0, 1, 0, 0, 0, 0, 0  …  0, 0, 0, 1, 0, 0, 0, 0, 0, 1]\n",
       " [1, 0, 0, 1, 1, 0, 1, 0, 1, 0  …  0, 0, 1, 0, 1, 0, 0, 0, 1, 1]\n",
       " [0, 0, 1, 0, 1, 0, 1, 1, 1, 0  …  1, 1, 1, 1, 0, 0, 1, 0, 1, 0]\n",
       " [1, 0, 1, 0, 1, 0, 0, 0, 1, 0  …  1, 0, 0, 1, 1, 1, 1, 0, 0, 1]\n",
       " [0, 0, 0, 1, 1, 1, 0, 1, 0, 1  …  1, 0, 1, 1, 1, 0, 1, 0, 0, 0]\n",
       " [0, 0, 1, 1, 0, 0, 0, 1, 1, 0  …  0, 0, 1, 1, 0, 0, 1, 1, 1, 0]\n",
       " [0, 1, 0, 0, 1, 1, 0, 1, 0, 1  …  1, 0, 0, 1, 0, 0, 1, 0, 0, 1]\n",
       " ⋮\n",
       " [1, 0, 0, 1, 0, 0, 0, 0, 0, 0  …  0, 1, 1, 1, 0, 1, 1, 0, 1, 0]\n",
       " [0, 0, 1, 0, 1, 0, 0, 0, 1, 0  …  0, 0, 0, 0, 1, 0, 1, 1, 0, 0]\n",
       " [1, 0, 1, 1, 0, 0, 1, 1, 0, 0  …  1, 0, 1, 0, 0, 0, 0, 1, 0, 1]\n",
       " [0, 0, 1, 0, 0, 1, 0, 0, 1, 1  …  1, 0, 1, 1, 0, 0, 0, 1, 1, 0]\n",
       " [1, 1, 0, 0, 1, 1, 1, 1, 1, 0  …  1, 0, 0, 1, 1, 0, 0, 0, 0, 1]\n",
       " [0, 0, 0, 1, 0, 1, 0, 1, 0, 0  …  1, 1, 0, 0, 1, 0, 1, 1, 1, 0]\n",
       " [1, 0, 0, 1, 1, 0, 0, 0, 0, 0  …  0, 1, 0, 0, 1, 0, 0, 0, 0, 1]\n",
       " [1, 1, 0, 1, 0, 0, 1, 0, 0, 0  …  1, 0, 0, 0, 0, 1, 0, 1, 1, 0]\n",
       " [0, 1, 0, 0, 0, 1, 1, 0, 0, 1  …  0, 0, 0, 1, 1, 1, 0, 0, 0, 0]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataloader loading\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataloader loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Epoch 1\n",
      "└ @ Main /home/mfat/.julia/packages/Flux/FWgS0/src/optimise/train.jl:185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_all(train_data, m) = 1.9785403f0\n",
      "loss_all(train_data, m) = "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.811339f0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_all(train_data, m) = 1.7997732f0\n",
      "loss_all(train_data, m) = "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7791259f0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_all(train_data, m) = 1.7759331f0\n"
     ]
    },
    {
     "ename": "InterruptException",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:\n",
      "\n",
      "Stacktrace:\n",
      "  [1] Array\n",
      "    @ ./boot.jl:461 [inlined]\n",
      "  [2] Array\n",
      "    @ ./boot.jl:469 [inlined]\n",
      "  [3] similar\n",
      "    @ ./array.jl:378 [inlined]\n",
      "  [4] *\n",
      "    @ ~/.julia/juliaup/julia-1.8.5+0.x64.linux.gnu/share/julia/stdlib/v1.8/LinearAlgebra/src/matmul.jl:148 [inlined]\n",
      "  [5] (::Dense{typeof(relu), Matrix{Float32}, Vector{Float32}})(x::BitMatrix)\n",
      "    @ Flux ~/.julia/packages/Flux/FWgS0/src/layers/basic.jl:174\n",
      "  [6] macro expansion\n",
      "    @ ~/.julia/packages/Flux/FWgS0/src/layers/basic.jl:53 [inlined]\n",
      "  [7] _applychain\n",
      "    @ ~/.julia/packages/Flux/FWgS0/src/layers/basic.jl:53 [inlined]\n",
      "  [8] (::Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}})(x::BitMatrix)\n",
      "    @ Flux ~/.julia/packages/Flux/FWgS0/src/layers/basic.jl:51\n",
      "  [9] loss_all(dataloader::DataLoader{Tuple{BitMatrix, OneHotArrays.OneHotMatrix{UInt32, Vector{UInt32}}}, Random._GLOBAL_RNG, Val{nothing}}, model::Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}})\n",
      "    @ Main ~/Master-Thesis/notebooks/mlp.ipynb:4\n",
      " [10] macro expansion\n",
      "    @ ./show.jl:1047 [inlined]\n",
      " [11] (::var\"#128#130\"{Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, DataLoader{Tuple{BitMatrix, OneHotArrays.OneHotMatrix{UInt32, Vector{UInt32}}}, Random._GLOBAL_RNG, Val{nothing}}})()\n",
      "    @ Main ~/Master-Thesis/notebooks/mlp.ipynb:12\n",
      " [12] macro expansion\n",
      "    @ ~/.julia/packages/Flux/FWgS0/src/optimise/train.jl:149 [inlined]\n",
      " [13] macro expansion\n",
      "    @ ~/.julia/packages/ProgressLogging/6KXlp/src/ProgressLogging.jl:328 [inlined]\n",
      " [14] train!(loss::Function, ps::Zygote.Params{Zygote.Buffer{Any, Vector{Any}}}, data::DataLoader{Tuple{BitMatrix, OneHotArrays.OneHotMatrix{UInt32, Vector{UInt32}}}, Random._GLOBAL_RNG, Val{nothing}}, opt::Adam; cb::var\"#128#130\"{Chain{Tuple{Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}, Dense{typeof(identity), Matrix{Float32}, Vector{Float32}}}}, DataLoader{Tuple{BitMatrix, OneHotArrays.OneHotMatrix{UInt32, Vector{UInt32}}}, Random._GLOBAL_RNG, Val{nothing}}})\n",
      "    @ Flux.Optimise ~/.julia/packages/Flux/FWgS0/src/optimise/train.jl:140\n",
      " [15] macro expansion\n",
      "    @ ~/.julia/packages/Flux/FWgS0/src/optimise/train.jl:186 [inlined]\n",
      " [16] macro expansion\n",
      "    @ ~/.julia/packages/ProgressLogging/6KXlp/src/ProgressLogging.jl:470 [inlined]\n",
      " [17] train(; kws::Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})\n",
      "    @ Main ~/Master-Thesis/notebooks/mlp.ipynb:15\n",
      " [18] train()\n",
      "    @ Main ~/Master-Thesis/notebooks/mlp.ipynb:1\n",
      " [19] top-level scope\n",
      "    @ ~/Master-Thesis/notebooks/mlp.ipynb:24"
     ]
    }
   ],
   "source": [
    "function train(; kws...)\n",
    "    # Initializing Model parameters \n",
    "    args = Args(; kws...)\n",
    "    println(\"dataloader loading\")\n",
    "    train_data,test_data = getdata(args)\n",
    "    println(\"dataloader loaded\")\n",
    "    m = build_model()\n",
    "    loss(x,y) = logitcrossentropy(m(x), y)\n",
    "    \n",
    "    ## Training\n",
    "    println(\"training started\")\n",
    "    evalcb = () -> @show(loss_all(train_data, m))\n",
    "    opt = Adam(args.rate)\n",
    "\t\t\n",
    "    @epochs args.epochs Flux.train!(loss, params(m), train_data, opt, cb = evalcb)\n",
    "\n",
    "    @show accuracy(train_data, m)\n",
    "\n",
    "    @show accuracy(test_data, m)\n",
    "\n",
    "    @save \"../data/model25_test.jld\" m\n",
    "end\n",
    "\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
