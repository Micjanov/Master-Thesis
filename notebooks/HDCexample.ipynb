{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showcase of the basics on Hyperdimensional Computing\n",
    "## What is it and why?\n",
    "Hyperdimensional computing is a relatively new paradigm of computing that tries to mimick the workings of a (human) brain (Kanerva). The brain has a massive amount of circuits made of neurons synapses which might suggest that our brains operate on very high dimensional vectors (HDV) of let's say of 10000 of even more bits. This is radically different from the typical modern computer architectures which operate on vectors of 8 to 64 bits. This becomes clear when we compare how easy it is for a human to learn a language compared to computers, which require a large and complecated set of arithmetic operations in the form of machine learning models or deep learning networks together with a large amount of data to try to come close mastering a language whilst a human can recognize relatively easy other languages when they don't even speak it! Likewise languages, we can very easily memorize and compare other intrisically complex and contextual concepts such as images. A computer would have a hard time finding similarity between a set of images because this requires very complex man-made models.\n",
    "\n",
    "When computing with hyperdimensional vectors, we only use a set of a few simple arithmetic operations which will all be explained later on in more detail. A HDV can represent anything from a word or a concept. This vector is initially made up of totally random bits, but with the simple set of rules, we can use other vectors to combine some concepts into new similar or dissimilar concepts. For example, we would not say that an table is similar to a brocolli but we could say that we can trace back the concept of table to the concept of meal, which in turn can be traced back to 'food' which has in some form similarity with 'brocolli'\n",
    "\n",
    "This holistic representation of a concept smeared out over a vector consisting of thousands of bits gives rise to interesting properties of this paradigm. First, it is very tolerant to failure of bits which makes it very robust to noise. This is becaue one bit in a HDV plays a very minimal role and may even be redundant for the represented concept, unlike in modern computer architectures where every bit in a 64 bit vector counts and cannot be different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating HDVs\n",
    "First we should create functions that lets us construct HDVs. An initial HDV is constructed totally random. We can opt for binary, bipolar vectors or even vectors containing real numbers. The nature of the vectors does not matter that much result-wise but using these operations on bipolar vectors will make it more understandable and is easier to implement. The choice of the nature of the vectors would also change the nature of the operations as we could use highly efficient bit-operations with binary bits but this heavily restricts the amount of information we can store in the vector as we explain later. The elements of the vectors will still be referred to as 'bits'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With n corresponding to the amount of vectors and N the length of the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hdv (generic function with 3 methods)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hdv(n::Int=1, N::Int=10000) = rand((-1,1), n, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×10000 Matrix{Int64}:\n",
       " 1  1  1  1  1  -1  1  -1  -1  -1  -1  …  -1  1  -1  -1  -1  1  -1  1  1  1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = hdv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×10000 Matrix{Int64}:\n",
       " 1  -1  1  1  -1  -1  1  1  -1  -1  1  …  1  -1  -1  1  1  1  1  1  1  -1  1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = hdv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations on HDVs\n",
    "In this part we will try to explain the different operations we could perform on HDVs which we could use for further modeling.\n",
    "### Adding (bundling)\n",
    "The element-wise addition of two of these kind of vectors result in a vector that is the most similar to both vectors. This result should also be normalized and and can be thus considered as a mean vector of the bundled vectors.(Kanerva)??? We can choose how far we want to go with this. If we allow zeros in our 'bipolar' vectors, we can easily see the disagreement of the corresponding elements of the strictly bipolar parent vectors. This also means that no information is lost if we don't restrict the resulting vectors to a strict bipolar nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "add (generic function with 2 methods)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add(vectors::Matrix{Int}...) = reduce(.+, vectors) .|> sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×10000 Matrix{Int64}:\n",
       " 1  0  1  1  0  -1  1  0  -1  -1  0  …  1  0  -1  0  0  0  0  1  0  1  0  1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×10000 Matrix{Int64}:\n",
       " 1  -1  -1  -1  1  -1  -1  -1  1  1  1  …  -1  -1  1  1  1  1  -1  1  -1  1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = hdv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×10000 Matrix{Int64}:\n",
       " 1  -1  1  1  1  -1  1  -1  -1  -1  1  …  -1  -1  1  1  1  1  -1  1  -1  1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add(x,y,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplying (binding)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
