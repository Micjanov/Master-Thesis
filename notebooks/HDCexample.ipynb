{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Showcase of the basics on Hyperdimensional Computing\n",
    "## What is it and why?\n",
    "Hyperdimensional computing is a relatively new paradigm of computing that tries to mimick the workings of a (human) brain (Kanerva). The brain has a massive amount of circuits made of neurons synapses which might suggest that our brains operate on very high dimensional vectors (HDV) of let's say of 10000 of even more bits. This is radically different from the typical modern computer architectures which operate on vectors of 8 to 64 bits. This becomes clear when we compare how easy it is for a human to learn a language compared to computers, which require a large and complecated set of arithmetic operations in the form of machine learning models or deep learning networks together with a large amount of data to try to come close mastering a language whilst a human can recognize other languages relatively easy when they don't even speak it! Likewise languages, we can very easily memorize and compare other intrisically complex and contextual concepts such as images. A computer would have a hard time finding similarity between a set of images and faces because this requires very complex man-made models.\n",
    "\n",
    "When computing with hyperdimensional vectors, we only use a set of a few simple arithmetic operations which will all be explained later on in more detail. A HDV can represent anything from a word or a concept. This vector is initially made up of totally random bits, but with the simple set of rules, we can use other vectors to combine some concepts into new similar or dissimilar concepts. For example, we would not say that an table is similar to a brocolli but we could say that we can trace back the concept of table to the concept of meal, which in turn can be traced back to 'food' which has in some form similarity with 'brocolli'\n",
    "\n",
    "This holistic representation of a concept smeared out over a vector consisting of thousands of bits gives rise to interesting properties of this paradigm. It is very tolerant to failure of bits which makes it very robust to noise. This is becaue one bit in a HDV plays a very minimal role and may even be redundant for the represented concept, unlike in modern computer architectures where every bit in a 64 bit vector counts and cannot be changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating HDVs\n",
    "First we should create functions that lets us construct HDVs. An initial HDV is constructed totally random. We can opt for binary, bipolar vectors or even vectors containing real numbers. The nature of the vectors does not matter that much result-wise but using these operations on bipolar vectors will make it more understandable and is easier to implement since we will then use simple algebraic concepts. The choice of the nature of the vectors would also change the nature of the operations as we could use highly efficient bit-operations with binary bits but this heavily restricts the amount of information we can store in the vector as we explain later.(??) The elements of the vectors will still be referred to as 'bits'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With n corresponding to the amount of vectors and N the length of the vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hdv (generic function with 3 methods)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using LinearAlgebra\n",
    "hdv(n::Int=1, N::Int=10000) = rand((-1,1), n, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×10000 Matrix{Int64}:\n",
       " -1  -1  -1  -1  1  1  1  1  -1  -1  1  …  1  -1  -1  -1  1  -1  -1  1  1  1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = hdv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×10000 Matrix{Int64}:\n",
       " 1  1  1  1  1  1  -1  1  1  -1  1  …  -1  1  -1  -1  1  1  -1  1  1  1  1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = hdv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations on HDVs\n",
    "In this part we will try to explain the different operations we could perform on HDVs which we could use for further modeling.\n",
    "### Adding (bundling)\n",
    "The element-wise addition of two of these kind of vectors result in a vector that is the most similar to both vectors. This result should also be normalized and and can be thus considered as a mean vector of the bundled vectors.(Kanerva)??? We can choose how far we want to go with this. If we allow zeros in our 'bipolar' vectors, we can easily see the disagreement of the corresponding elements of the strictly bipolar parent vectors. This also means that no information is lost if we don't restrict the resulting vectors to a strict bipolar nature. (What kind of operation would be usuable for binary vectors?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "add (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add(vectors::Matrix{Int}...) = reduce(.+, vectors) .|> sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×10000 Matrix{Int64}:\n",
       " 0  0  0  0  1  1  0  1  0  -1  1  -1  …  -1  1  -1  -1  0  1  -1  0  1  1  1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×10000 Matrix{Int64}:\n",
       " -1  -1  1  -1  -1  1  1  -1  1  1  1  …  1  1  1  -1  1  1  1  -1  1  -1  -1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = hdv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×10000 Matrix{Int64}:\n",
       " -1  -1  1  -1  1  1  1  1  1  -1  1  -1  …  1  -1  -1  1  1  -1  -1  1  1  1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add(x,y,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplying (binding)\n",
    "This operation encodes the interaction between two vectors and thus creates a vector dissimilar to the two parent vectors. For bipolar parent vectors, element-wise multiplication suffices. For strictly bipolar vectors, this also implies we can reverse the binding operation by doing the same operation again thus recovering the data. For binary vectors, this operation corresponds to a logical XOR gate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "multiply (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiply(vectors::Matrix{Int}...) = reduce(.*, vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×10000 Matrix{Int64}:\n",
       " -1  -1  -1  -1  1  1  -1  1  -1  1  1  …  1  1  1  1  -1  1  1  -1  1  1  1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiply(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×10000 Matrix{Int64}:\n",
       " 1  1  -1  1  -1  1  -1  -1  -1  1  1  …  1  1  1  -1  -1  1  1  1  1  -1  -1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiply(x, y, z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation (shifting)\n",
    "We can also include positional information, for example a sequence of letters. This is possible by a cyclic permutation by shifting every element to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "perm (generic function with 2 methods)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perm(vector::Matrix{Int}, k::Int=1) = circshift(vector, (0, k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5028"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(x.==perm(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×10000 Matrix{Int64}:\n",
       " -1  -1  -1  -1  1  1  1  1  -1  -1  1  …  1  -1  -1  -1  1  -1  -1  1  1  1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circshift(x, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to encode the sequence of letters 'xyz' into one vector, we can bind the vectors of each letter but shift the vectors according to their index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×10000 Matrix{Int64}:\n",
       " 1  1  1  1  1  -1  -1  -1  -1  1  -1  …  -1  -1  1  1  -1  -1  1  1  -1  1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xyz = multiply(x, perm(y), perm(z, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity measurement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quantify the similarity between two HDVs, we can tackle it in numerous ways depending on the nature of the vectors. For bipolar vectors, a cosine similarity measure fits our needs. \n",
    "$$\\cos(\\mathbf{x}, \\mathbf{y}) = \\frac{\\mathbf{x} \\cdot \\mathbf{y}}{||\\mathbf{x}||\\, ||\\mathbf{y}||}\\,.$$\n",
    "If the cosine similairty is equal to -1, the two vectors are totally opposite to eachother. If it's equal to 1, the vectors are equal and if the cosine similarity equals to 0, the vectors are orthogonal to eachother."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cosine (generic function with 2 methods)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(x::Matrix{Int}, y::Matrix{Int}) = dot(x, y) / (norm(x) * norm(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see and expect, 2 unrelated random vectors are close to being orthogonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.002"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(x, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
